{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우는\n",
    "1. 그래프 생성\n",
    "2. 그래프 실행\n",
    "\n",
    "으로 분리되있다!\n",
    "\n",
    "이런방식을 lazy evaluation(지연 실행) 이라 하고, 함수형 프로그래밍에서 많이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder와 Variable\n",
    "\n",
    "Placeholder : 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수(parameter)\n",
    "\n",
    "Variable : 그래프를 최적화하는 용도로 텐서플로가(더 정확히는 학습 함수들이) 학습한 결과를 갱신하기 위해 사용하는 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# None은 크기가 정해지지 않았음을 의미한다.\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[ 0.06925105  0.37156472]\n",
      " [ 0.35041285  2.0497444 ]\n",
      " [-0.7118697   1.7890985 ]]\n",
      "=== b ===\n",
      "[[-0.50515264]\n",
      " [ 0.46107227]]\n",
      "=== expr ===\n",
      "[[-1.8706851  9.333196 ]\n",
      " [-1.7810776 22.930643 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function은 한 쌍(x, y)의 데이터에 대한 **손실값**을 계산하는 함수이다.\n",
    "손실값은 실제값과 모델로 예측한 값이 얼마나 ㅊ아이가 나는가를 나타내는 뜻, 손실값이 작을수록 그 모델이 X와 Y의 관계를 잘 설명하고 있다는 의미이다.\n",
    "\n",
    "즉, **학습**이란 변수들의 값을 다양하게 넣어 계산해 보면서 이 손실값을 최소화하는 W와 b를 구하는 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate를 크게하면 최적의 손실값을 찾지 못하고 지나치게 되고, 값이 너무 작으면 학습 속도가 매우 느려진다. 이렇게 학습을 진행하는 과정에 영향을 주는 변수를 **hyperparameter(하이퍼파라미터)**라고 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8689479 [0.93024176] [0.31560108]\n",
      "1 0.034249913 [0.86910903] [0.28038418]\n",
      "2 0.011767693 [0.8791203] [0.27666375]\n",
      "3 0.010959578 [0.8812759] [0.26968288]\n",
      "4 0.010436017 [0.8842119] [0.26323596]\n",
      "5 0.009940264 [0.88698643] [0.256904]\n",
      "6 0.009468083 [0.88970417] [0.25072864]\n",
      "7 0.009018361 [0.8923555] [0.24470124]\n",
      "8 0.008589971 [0.89494324] [0.2388188]\n",
      "9 0.008181936 [0.8974687] [0.23307773]\n",
      "10 0.0077932887 [0.89993346] [0.2274747]\n",
      "11 0.007423099 [0.902339] [0.22200638]\n",
      "12 0.0070705004 [0.90468675] [0.21666953]\n",
      "13 0.0067346417 [0.90697795] [0.21146092]\n",
      "14 0.0064147487 [0.90921414] [0.20637755]\n",
      "15 0.0061100367 [0.91139656] [0.20141639]\n",
      "16 0.005819809 [0.91352654] [0.19657448]\n",
      "17 0.0055433665 [0.9156053] [0.19184898]\n",
      "18 0.005280055 [0.91763407] [0.18723705]\n",
      "19 0.00502925 [0.9196141] [0.18273601]\n",
      "20 0.0047903596 [0.9215466] [0.17834319]\n",
      "21 0.0045628017 [0.9234324] [0.1740559]\n",
      "22 0.004346073 [0.9252731] [0.16987176]\n",
      "23 0.004139631 [0.92706954] [0.16578817]\n",
      "24 0.003943001 [0.9288227] [0.16180272]\n",
      "25 0.0037557 [0.93053377] [0.1579131]\n",
      "26 0.0035773038 [0.93220365] [0.15411697]\n",
      "27 0.0034073854 [0.93383354] [0.15041214]\n",
      "28 0.0032455297 [0.9354241] [0.1467963]\n",
      "29 0.003091364 [0.93697643] [0.14326741]\n",
      "30 0.0029445172 [0.93849146] [0.13982336]\n",
      "31 0.0028046442 [0.93997] [0.13646209]\n",
      "32 0.0026714269 [0.9414131] [0.13318165]\n",
      "33 0.002544538 [0.94282156] [0.12998009]\n",
      "34 0.0024236615 [0.9441961] [0.12685545]\n",
      "35 0.0023085475 [0.9455376] [0.12380593]\n",
      "36 0.0021988796 [0.9468468] [0.12082969]\n",
      "37 0.0020944339 [0.9481246] [0.11792504]\n",
      "38 0.0019949495 [0.94937164] [0.11509021]\n",
      "39 0.001900186 [0.9505887] [0.11232352]\n",
      "40 0.0018099259 [0.9517765] [0.10962333]\n",
      "41 0.0017239507 [0.95293576] [0.10698805]\n",
      "42 0.0016420633 [0.9540672] [0.10441614]\n",
      "43 0.0015640688 [0.95517135] [0.10190604]\n",
      "44 0.0014897707 [0.956249] [0.09945629]\n",
      "45 0.0014190054 [0.9573007] [0.09706543]\n",
      "46 0.0013515977 [0.95832723] [0.09473205]\n",
      "47 0.0012874033 [0.959329] [0.09245475]\n",
      "48 0.0012262514 [0.96030676] [0.09023221]\n",
      "49 0.0011679988 [0.9612609] [0.08806306]\n",
      "50 0.001112518 [0.9621922] [0.08594608]\n",
      "51 0.00105967 [0.963101] [0.08387998]\n",
      "52 0.001009334 [0.96398807] [0.08186357]\n",
      "53 0.0009613904 [0.96485376] [0.07989563]\n",
      "54 0.0009157244 [0.96569866] [0.07797498]\n",
      "55 0.00087222754 [0.9665233] [0.07610053]\n",
      "56 0.0008307921 [0.96732795] [0.0742711]\n",
      "57 0.0007913336 [0.9681134] [0.07248569]\n",
      "58 0.00075374404 [0.96887994] [0.07074319]\n",
      "59 0.0007179423 [0.9696281] [0.06904259]\n",
      "60 0.00068383664 [0.97035813] [0.06738282]\n",
      "61 0.0006513553 [0.97107077] [0.065763]\n",
      "62 0.0006204153 [0.9717662] [0.0641821]\n",
      "63 0.0005909473 [0.9724449] [0.06263921]\n",
      "64 0.0005628771 [0.97310734] [0.06113341]\n",
      "65 0.0005361375 [0.9737538] [0.0596638]\n",
      "66 0.00051067205 [0.9743848] [0.05822953]\n",
      "67 0.00048641497 [0.97500056] [0.05682972]\n",
      "68 0.0004633105 [0.9756015] [0.05546355]\n",
      "69 0.00044129835 [0.976188] [0.05413025]\n",
      "70 0.0004203373 [0.9767604] [0.05282899]\n",
      "71 0.00040037304 [0.97731906] [0.05155903]\n",
      "72 0.00038135494 [0.9778643] [0.0503196]\n",
      "73 0.0003632387 [0.9783964] [0.04910994]\n",
      "74 0.00034598587 [0.9789158] [0.04792939]\n",
      "75 0.00032955103 [0.9794226] [0.04677719]\n",
      "76 0.00031389596 [0.9799173] [0.0456527]\n",
      "77 0.00029898834 [0.9804001] [0.04455525]\n",
      "78 0.00028478427 [0.98087126] [0.04348417]\n",
      "79 0.00027125594 [0.98133105] [0.04243883]\n",
      "80 0.00025837365 [0.9817799] [0.04141866]\n",
      "81 0.00024609957 [0.98221785] [0.04042298]\n",
      "82 0.00023441017 [0.98264533] [0.03945124]\n",
      "83 0.00022327671 [0.9830625] [0.03850286]\n",
      "84 0.00021266862 [0.98346967] [0.03757728]\n",
      "85 0.00020256803 [0.98386705] [0.03667395]\n",
      "86 0.00019294558 [0.9842549] [0.03579234]\n",
      "87 0.00018377903 [0.9846334] [0.03493192]\n",
      "88 0.00017505226 [0.9850028] [0.03409218]\n",
      "89 0.00016673429 [0.9853633] [0.03327262]\n",
      "90 0.00015881524 [0.98571515] [0.03247276]\n",
      "91 0.00015127107 [0.98605853] [0.03169214]\n",
      "92 0.00014408614 [0.98639375] [0.0309303]\n",
      "93 0.0001372425 [0.9867208] [0.03018673]\n",
      "94 0.00013072367 [0.98704004] [0.02946107]\n",
      "95 0.00012451266 [0.98735154] [0.02875284]\n",
      "96 0.00011859811 [0.98765564] [0.02806165]\n",
      "97 0.00011296533 [0.9879524] [0.02738707]\n",
      "98 0.000107599444 [0.98824203] [0.02672869]\n",
      "99 0.000102489816 [0.9885247] [0.02608616]\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [4.9687095]\n",
      "X: 2.5, Y: [2.4973977]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print('\\n=== Test ===')\n",
    "    print('X: 5, Y:', sess.run(hypothesis, feed_dict={X:5}))\n",
    "    print('X: 2.5, Y:', sess.run(hypothesis, feed_dict={X:2.5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
